1. Logistic Regression:
Intro- Model for Classification.
Adv- It is useful for understanding the influence of several Independent variables on a single outcome variable.
Disadv- Assumes data is free of missing values.

2. Naïve Bayes:
Intro- It is based on Bayes’ theorem.
Adv- Useful for Spam Filtering, Document Classification. It is fast and results even on small amount of training data.
Disadv- Known to be Bad Estimator.

3. Stochastic Gradient Descent:
Intro- Simple and efficient model approach to fit linear models.
Useful when number of samples are very high, supports penalties and loss functions for classification.
Adv- Simple to implement.
Disadv- Requires large number of data, sensitive to feature scaling.

4. K-Nearest Neighbours: 
Intro- Lazy learning type of module, it just stores
instance of data and does not build internal model of training data. Result is
generated by majority of nearest votes. 
Adv- Simple to implement and effective
in large data, filters noise. 
Disadv- Need to determine the value of K and Needs high computation cost as it needs
to compute the distance of each instance to all the training samples.

5. Decision Tree:
Intro- Decision tree produces sequence of rules that can be used to classify the data.
Adv- Numberical as well as categorical data.
Disadv- Can direct into complex tree structure that do not generalise well and unstable even if small variations occur.

6. Random Forest:
Intro- Uses average to improve accuracy. It works like a meta-estimator which fits number of decision trees on various sub-samples of datasets.
Adv- Controls overfitting, more accurate then decision tree in most cases.
Disadv- Slow, difficult to implement, complex algorithm.

7. Support Vector Machine:
Intro- Considers training data as points in space seperated categories by a considerable gap. 
Samples are predicted to a category based on which side of gap they points.
Adv- Memory Efficient. Best for high dimentional spaces where it uses subset of training points for the decision function.
Disadv- Does not directly provide probability estimates, they are calculated using an expensive five-fold cross-validation.


References:
1.Introduction - Learn Python for Data Science #1 -By Siraj Raval
https://youtu.be/T5pRlIbr6gg?list=PL2-dafEMk2A6QKz1mrk1uIGfHkC1zZ6UU

2.ANALYTICSINDIAMAG
https://analyticsindiamag.com/7-types-classification-algorithms/

3.Scikitlearn Documentation
https://scikitlearnjl.readthedocs.io/en/latest/models/#scikitlearn-models

4.scikit-learn.org
https://scikit-learn.org/dev/auto_examples/index.html#decision-trees

5.census.gov
https://www.census.gov/programs-surveys/decennial-census/2020-census/research-testing/testing-activities.html

6.GitHub
https://github.com/f2005636/Classification